---
title: "Weeks 5-10"
---

```{r LoadLibraries}
#| title: "Load Libraries"
#| messages: false
#| echo: true
#| error: false
#| warning: false
#| results: "hide"
library(knitr)
library(gtsummary)
library(tidyverse)
library(flextable)
library(kableExtra)
library(ggplot2)
library(gtsummary)
```



# Week 5

## Shoring up our understanding of the basics

Take the self-assessment please.

# Week 6

```{r OurData}
#| title: "Our Data"
#| messages: false
#| echo: true
#| error: false
data(mtcars)
str(mtcars)
mtcars2 <- within(mtcars, {
   vs.f <- factor(vs, labels = c("V", "S"))
   am.f <- factor(am, labels = c("automatic", "manual"))
   gear.f <- factor(gear, labels = c("3g", "4g", "5g"))
   cyl.f <- factor(cyl, labels = c("4c", "6c", "8c"))
   carb.f <- factor(carb, labels = c("1b", "2b", "3b", "4b", "6b", "8b"))
   cyl.fo  <- ordered(cyl)
   gear.fo <- ordered(gear)
   carb.fo <- ordered(carb)
})
knitr::kable(str(mtcars2))
```


## Categorical Predictors

This week (2/19) is our week to get cracking on categorical predictors. We'll start with a review of the basics, and then address two:
- Dummy coding
- Effects coding

Next week, we tackle contrast coding and polynomial coding.

### Some Basics

Categorical predictors are typically non-numerical.  In R, we refer to these predictors as factors.  Factors are a type of variable that takes on a limited number of different values; each value is referred to as a level.  For example, the variable `vs` in the `mtcars` dataset is a factor with two levels: `V` and `S`.  The variable `am` in the `mtcars` dataset is a factor with two levels: `automatic` and `manual`.  I created factors from two other variables - gear and cyl - and gave them labels that are more meaningful than the numbers that they represent.  All of these variables contain categorical data and, as a result, need to be converted into numbers so we can use them in the General Linear Model.


#### Levels, Tables, and Contrasts

You have two tools at your disposal to understand the $k$-levels of a factor: `levels()` and `table()`.  The `levels()` function returns the levels of a vector that has been stored as a factor (i.e., `as.factor()` or `factor()`.  The `table()` function returns the frequency of each level of a factor.  Additionally, the `contrasts()` function returns the contrasts used to code the levels of a factor.  We will use these functions to understand the levels of the factors we created from the `mtcars` dataset.

```{r CodingBasics}
#| title: "Coding Basics"
#| messages: false
#| echo: true
#| error: false
#| warning: false
levels(mtcars2$vs)
table(mtcars2$vs)
levels(mtcars2$vs.f)
levels(mtcars2$am)
table(mtcars2$am)
levels(mtcars2$am.f)
levels(mtcars2$gear.f)
levels(mtcars2$cyl.f)
contrasts(mtcars2$vs.f)
contrasts(mtcars2$gear.f)
contrasts(mtcars2$cyl.f)
```


### Dummy Coding

R defaults to dummy coding so if you prefer this method, just leave the handling of factors to R.  **NOT RECOMMENDED**.  Usually, we want to use a different coding method but for now, we will keep to dummy codes or, as referred to in the R world - treatment contrasts.

#### One 2-level Factor

When we have a factor with two levels, we get one new variable.  This new variable is a binary variable that takes on the value of 0 or 1.  The 0 is the reference level and the 1 is the other level.  The reference level is the first level of the factor unless otherwise specified.  The new variable is a Dummy code and, as a result, gets interpreted as such.  Always know the method by which a factor is coded.  You cannot interpret your regression results without knowing this tidbit.  So, ask.  Here, we are using dummy coding.  

The example below has a factor `vs.f` that is used to predict vehicle fuel efficiency (mpg).  


```{r DummyCoding2lvl}
#| messages: false
#| echo: true
#| error: false
#| warning: false

# Base R way
#contrasts(mtcars2$vs.f)
#fit <- lm(mpg ~ vs.f, data = mtcars2)
#summary(fit)

# Tidyverse way
mtcars2 %>%
  dplyr::as_tibble() %>%
  lm(mpg ~ vs.f, 
     data = .,
     contrasts = list(vs.f = contr.treatment)) %>%
  summary()

# means by vs.f
mtcars2 %>%
  group_by(vs.f) %>%
  summarise(mean_mpg = mean(mpg)) %>%
  kable(digits = 3)

```

Note how the `V` level of the `vs.f` factor is the reference level.  The coefficient for `vs.fS` is the difference between the mean of `mpg` for the `S` level of `vs.f` and the mean of `mpg` for the `V` level of `vs.f`.  The coefficient for `vs.fV` is actually the intercept.  So, the intercept is the reference level mean and the regression coefficient for each level of the factor is the **difference between** that reference mean.  Pretty easy, huh?

$$mpg = \beta_0 + \beta_1 * (vs.fS) + \epsilon = \hat{mgp} + \epsilon$$
or, for our results:

$$\hat{mpg} = 16.62 + 7.94 * (vs.fS)$$

and remember that `vs.fS` is our Dummy code.  Since V is coded as 0...

$$\hat{mpg}_{V} = 16.62 + 7.94 * (0) = 16.6$$

and since S is coded as 1...

$$\hat{mpg}_{S} = 16.62 + 7.94 * (1) = 24.56$$

So, the mean `mpg` for a vehicle with an S engine is 16.6 and the mean `mpg` for a vehicle with a V engine is 24.56.  The difference between the mean `mpg` for a vehicle with an S engine and a vehicle with a V engine is 7.94.  This is the coefficient for `vs.fS` in the regression equation.  The coefficient for `vs.fV` is the intercept.  Easy, right?

#### One k-level Factor

# RESUME HERE

Two-level factors are easy.  When we have two levels ($k=2$), we get one ($k-1=2-1=1$) new variables.  But what about a factor with three levels ($k=3$)?  We get two ($k-1$) or $3-1=2$ new variables.  The first dummy variable is the second (or first) comparison level.  This variable is coded as `1` for all those observations in group 2. ZZZZ is the difference between means of the first and second levels of the factor.   of the second level compared to the base level (often the first, unless otherwise specified).  The second variable is the difference between the mean of the third level and the mean of the third level.  The third level is the reference level.

```{r DummyCoding3lvl}
#| messages: false
#| echo: true
#| error: false
#| warning: false

# Base R way
contrasts(mtcars2$gear.f)
fit <- lm(mpg ~ gear.f, data = mtcars2)
summary(fit)

# Tidyverse way
mtcars2 %>%
  dplyr::as_tibble() %>%
  lm(mpg ~ gear.f, 
     data = .,
     contrasts = list(gear.f = contr.treatment)) %>%
  summary()

# means by gear.f
mtcars2 %>%
  group_by(gear.f) %>%
  summarise(mean_mpg = mean(mpg)) %>%
  kable(digits = 3)
```

#### Two Factors (Main Effects)

```{r Dummy2FactorsMainEffects}
#| messages: false
#| echo: true
#| error: false
#| warning: false
table(mtcars2$vs.f, mtcars2$cyl.f)

mtcars2 %>%
  dplyr::as_tibble() %>%
  lm(mpg ~ vs.f + cyl.f, 
     data = .,
     contrasts = list(vs.f = contr.treatment, gear.f = contr.treatment)) %>%
  summary()

# means by vs.f and gear.f
mtcars2 %>%
  group_by(vs.f, cyl.f) %>%
  summarise(mean_mpg = mean(mpg)) %>%
  kable(digits = 3)
```



#### Two Factors (Interaction)

```{r Dummy2FactorsFullFactorial}
#| messages: false
#| echo: true
#| error: false
#| warning: false
mtcars2 %>%
  dplyr::as_tibble() %>%
  lm(mpg ~ vs.f * cyl.f, 
     data = .,
     contrasts = list(vs.f = contr.treatment, cyl.f = contr.treatment)) %>%
  summary()

# means by vs.f and gear.f
mtcars2 %>%
  group_by(vs.f, cyl.f) %>%
  summarise(mean_mpg = mean(mpg)) %>%
  kable(digits = 3)
```



## Effects Coding

Effects coding is a bit more complicated.  The base comparison level of the factor is coded as $-1$; the focal level for each "new variable" is coded with a 1.  Recall that the same coding scheme exists for dummy coding but with effects coding, the LAST level is the default base comparison level and is coded with a $-1$ instead of a $0$.  The big deal about effects codes - at least compared to dummy coding - is that effects codes are centered AND orthogonal (kinda).  We will discuss these properties in a bit.  

### One Factor with 2 Levels


```{r EffCoding2lvl}
#| messages: false
#| echo: true
#| error: false
#| warning: false

# Tidyverse way
mtcars2 %>%
  dplyr::as_tibble() %>%
  lm(mpg ~ vs.f, 
     data = .,
     contrasts = list(vs.f = contr.sum)) %>%
  summary()

# means by vs.f
mtcars2 %>%
  group_by(vs.f) %>%
  summarise(mean_mpg = mean(mpg)) %>%
  kable(digits = 3)

# grand mean
mtcars2 %>%
  group_by(vs.f) %>%
  summarise(mean_mpg = mean(mpg)) %>%
  summarise(grandMean_mpg = mean(mean_mpg)) %>%
  kable(digits = 3)

```

### One k-level Factor

```{r EffCoding3lvl}
#| messages: false
#| echo: true
#| error: false
#| warning: false

mtcars2 %>%
  dplyr::as_tibble() %>%
  lm(mpg ~ cyl.f, 
     data = .,
     contrasts = list(cyl.f = contr.sum)) %>%
  summary()

# means by gear.f
mtcars2 %>%
  group_by(cyl.f) %>%
  summarise(mean_mpg = mean(mpg)) %>%
  kable(digits = 3)

# grand mean
mtcars2 %>%
  group_by(cyl.f) %>%
  summarise(mean_mpg = mean(mpg)) %>%
  summarise(grandMean_mpg = mean(mean_mpg)) %>%
  kable(digits = 3)
```

### Two Factors (Main Effects)

```{r Eff2FactorsMainEffects}
#| messages: false
#| echo: true
#| error: false
#| warning: false

mtcars2 %>%
  dplyr::as_tibble() %>%
  lm(mpg ~ vs.f + cyl.f, 
     data = .,
     contrasts = list(vs.f = contr.sum, cyl.f = contr.sum)) %>%
  summary()

# means by vs.f and gear.f
mtcars2 %>%
  group_by(vs.f, cyl.f) %>%
  summarise(mean_mpg = mean(mpg)) %>%
  kable(digits = 3)

# grand mean
mtcars2 %>%
  group_by(vs.f, cyl.f) %>%
  summarise(mean_mpg = mean(mpg)) %>%
  summarise(grandMean_mpg = mean(mean_mpg)) %>%
  kable(digits = 3)
```

### Two Factors (Interaction)

```{r Eff2FactorsFullFactorial}
#| messages: false
#| echo: true
#| error: false
#| warning: false

mtcars2 %>%
  dplyr::as_tibble() %>%
  lm(mpg ~ vs.f * cyl.f, 
     data = .,
     contrasts = list(vs.f = contr.sum, cyl.f = contr.sum)) %>%
  summary()

# means by vs.f and gear.f
mtcars2 %>%
  group_by(vs.f, cyl.f) %>%
  summarise(mean_mpg = mean(mpg)) %>%
  kable(digits = 3)
```



```{r othermodels}
#| messages: false
#| echo: true
#| error: false
#| warning: false





lm(mpg ~ 1, data = mtcars2) %>%
  tbl_regression(intercept = TRUE, add_estimate_to_reference_rows=TRUE) %>% 
  add_glance_table(include = c(r.squared, adj.r.squared)) %>% 
  bold_p(t = 0.05) %>% 
  as_flex_table()

lm(mpg ~ vs.f, data = mtcars2) %>%
  tbl_regression()

lm2 <- lm(mpg ~ am.f, data = mtcars2) %>%
  tbl_regression()

lm3 <- lm(mpg ~ gear.f, data = mtcars2) %>%
  tbl_regression()

lm4 <- lm(mpg ~ cyl.f, data = mtcars2) %>%
  tbl_regression()

```



### Effects Coding


# Week 7

## Categorical Predictors (cont.)

### Contrast Coding

### Polynomial Coding

# Week 8 - SPRING BREAK (No Class)

# Week 9

## Interactions (aka Moderation)

### Simple Interactions

### Complex Interactions

# Week 10

## Moderation (cont.)

### Interactions with Categorical Predictors

### Interactions with Multiple Predictors

### Interactions with Polynomial Predictors

### Interactions with Higher-Order Predictors

